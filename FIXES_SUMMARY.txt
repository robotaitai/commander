â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ğŸ”§ CRITICAL TRAINING FIXES APPLIED                       â•‘
â•‘                          January 26, 2026                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Fix #1: Added --n-steps-per-env Parameter
   â€¢ PPO rollout buffer now SCALES with n_envs (was constant at ~2048)
   â€¢ Example: 32 envs Ã— 256 steps = 8192 transitions/update (was 2048)
   â€¢ Impact: Actually increases batch size when you add more environments!

âœ… Fix #2: Fixed Batch Size Divisibility
   â€¢ New pick_batch_size() helper ensures batch_size divides buffer evenly
   â€¢ Prevents silent errors from invalid batch sizes
   â€¢ Maintains GPU efficiency (multiples of 64)

âœ… Fix #5: Fixed Rich Table Rendering
   â€¢ MetricsCallback now uses temporary Console for table printing
   â€¢ No conflicts with progress bar (separate console instances)
   â€¢ Tables render properly instead of showing object repr

âœ… Fix #3: Evaluation Config (Already Fixed)
   â€¢ evaluate.py correctly uses run's config snapshot
   â€¢ No config mismatch causing "high reward but lost"

âœ… Fix #4: GPU Utilization Reality
   â€¢ 10-25% GPU for single MLP job is NORMAL (not a bug!)
   â€¢ Rollout phase is CPU-bound (GPU idle 70-90% of time)
   â€¢ Solution: Run 3-4 parallel jobs for 50-70% GPU utilization

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        ğŸš€ HOW TO USE THE FIXES                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPTION 1: Single Training Job (Updated Params)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -m mission_gym.scripts.train_ppo \
  --timesteps 50000000 \
  --n-envs 32 \
  --n-steps-per-env 256 \     â† NEW! Controls batch size scaling
  --subproc \
  --network-arch "512,512,256" \
  --n-epochs 20 \
  --parent-checkpoint runs/2-defenders-gpu-beast-20260125-234819/checkpoints/ppo_mission_34724352_steps \
  --branch-name "proper-batch" \
  --notes "Using fixed n-steps-per-env for proper batch scaling"

â€¢ Rollout buffer: 32 Ã— 256 = 8192 (was 2048!)
â€¢ Batch size: 2048 (4 minibatches, guaranteed divisible)
â€¢ GPU util: ~15-20% (expected for single job)


OPTION 2: Parallel Jobs for GPU Saturation ğŸ”¥ (RECOMMENDED)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cd /home/itai/code/commander
./add_parallel_jobs.sh

â€¢ Starts 3 additional training jobs (4 total)
â€¢ Each tests different config (large batch, wide net, deep net)
â€¢ GPU util: 50-70% (sustained!)
â€¢ Logs: logs/train_*.log

Monitor:
  watch -n 2 nvidia-smi
  watch -n 2 './monitor_parallel.sh'


OPTION 3: Fresh Start with 4 Parallel Jobs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cd /home/itai/code/commander
./parallel_train.sh

â€¢ Stops any existing training
â€¢ Starts 4 optimized jobs from scratch
â€¢ Each job: 24-32 envs, 256 steps/env, 20-30 epochs
â€¢ GPU util: 50-70%

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           ğŸ“Š WHAT CHANGED                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Files Modified:
  â€¢ mission_gym/scripts/train_ppo.py
    - Added --n-steps-per-env argument (default: 128)
    - Added pick_batch_size() helper function
    - Fixed n_steps calculation (no longer divided by n_envs)
    - Added PPO config printout during training start
  
  â€¢ parallel_train.sh & add_parallel_jobs.sh
    - All jobs now use --n-steps-per-env
    - Optimized for GPU saturation
  
  â€¢ TRAINING_CHEATSHEET.md
    - Updated all examples with --n-steps-per-env
    - Corrected GPU utilization expectations
    - Added parallel training workflows
  
  â€¢ docs/CRITICAL_TRAINING_FIXES.md (NEW)
    - Full technical explanation of all 4 fixes

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        ğŸ¯ RECOMMENDED NEXT STEPS                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. âœ… Stop your current training (if using old params)
   Ctrl+C in the training terminal

2. âœ… Start parallel training for GPU saturation:
   cd /home/itai/code/commander
   ./add_parallel_jobs.sh

3. âœ… Monitor GPU utilization (should be 50-70%):
   watch -n 1 nvidia-smi

4. âœ… Check logs after 5-10 minutes:
   tail -f logs/train_large_batch.log
   tail -f logs/train_wide.log
   tail -f logs/train_deep.log

5. âœ… View dashboards:
   firefox runs/dashboard.html

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          ğŸ“– DOCUMENTATION                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â€¢ TRAINING_CHEATSHEET.md           - Quick reference (updated)
  â€¢ docs/CRITICAL_TRAINING_FIXES.md  - Detailed explanation (NEW)
  â€¢ docs/GPU_TRAINING_GUIDE.md       - Full GPU training guide

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                             âš ï¸ IMPORTANT                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  From now on, ALWAYS use --n-steps-per-env when training!

  Default (if omitted): --n-steps-per-env 128
  Recommended:          --n-steps-per-env 256

  Old behavior (n_steps = 2048 // n_envs) is GONE and was incorrect!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                           Ready to train properly! ğŸš€
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
