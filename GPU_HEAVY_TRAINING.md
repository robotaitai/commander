# GPU-Heavy Training Guide

## Quick Start: Use Presets!

```bash
# Fast preset (good for testing)
python -m mission_gym.scripts.train_ppo --preset fast --timesteps 50000000

# Heavy preset (recommended for training)
python -m mission_gym.scripts.train_ppo --preset heavy --timesteps 50000000

# Beast preset (maximum GPU utilization)
python -m mission_gym.scripts.train_ppo --preset beast --timesteps 50000000
```

## Preset Configurations

| Preset | n_envs | n_steps | Total Batch | Batch Size | Epochs | Network | GPU Util | FPS |
|--------|--------|---------|-------------|------------|--------|---------|----------|-----|
| **fast** | 16 | 256 | 4096 | 2048 | 15 | 512,512 | ~30% | ~800 |
| **heavy** â­ | 32 | 256 | 8192 | 4096 | 20 | 1024,512,256 | ~50% | ~700 |
| **beast** ðŸ”¥ | 64 | 512 | 32768 | 8192 | 30 | 1024,512,512,256 | ~80% | ~600 |

---

## What Changed (Technical Details)

### 1. Decoupled n_steps from n_envs

**Before (BROKEN):**
```python
n_steps = 2048 // n_envs  # Keeps total batch constant!
```

**After (FIXED):**
```python
n_steps = args.n_steps  # Independent parameter
total_batch = n_envs * n_steps  # Now scales properly!
```

**Impact:** Can actually increase batch size by adding more environments!

### 2. Large Batch Sizes for GPU

**Before:** batch_size ~256-512 (tiny!)

**After:** batch_size 2048-8192 (proper GPU saturation)

Uses `choose_batch_size()` helper that picks largest valid divisor.

### 3. More Epochs Per Rollout

**Before:** 10 epochs (default)

**After:** 15-30 epochs depending on preset

More GPU compute per rollout = better utilization.

### 4. Bigger Networks

**Before:** 256,256 (small MLP)

**After:** 512,512 to 1024,512,512,256 (much larger)

More parameters = more GPU work.

### 5. Enabled TF32 on RTX 4070

```python
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision("high")
```

**Impact:** ~2x faster matmul with negligible accuracy loss!

### 6. True Parallelism with SubprocVecEnv

**Before:** DummyVecEnv (sequential)

**After:** SubprocVecEnv with forkserver (true parallel)

```python
envs = SubprocVecEnv([make_env(i) for i in range(n_envs)], 
                     start_method="forkserver")
```

### 7. Prevent CPU Thread Storms

```python
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
torch.set_num_threads(1)
```

Prevents CPU threads from starving subprocess workers.

---

## Manual Configuration

If you don't want presets, use these flags:

```bash
python -m mission_gym.scripts.train_ppo \
  --timesteps 50000000 \
  --n-envs 32 \
  --n-steps 256 \
  --batch-size 4096 \
  --n-epochs 20 \
  --network-arch "1024,512,256" \
  --subproc \
  --run-name my-heavy-run
```

**Key parameters:**
- `--n-envs`: Number of parallel environments (16/32/64)
- `--n-steps`: Steps per environment per rollout (256/512)
- `--batch-size`: PPO minibatch size (2048/4096/8192) - must divide total_batch!
- `--n-epochs`: Training epochs per rollout (15/20/30)
- `--network-arch`: MLP layer sizes (comma-separated)

---

## Monitoring GPU Properly

**DON'T use 1-second snapshots:**
```bash
watch -n 1 nvidia-smi  # âŒ Misses bursts!
```

**DO use high-frequency monitoring:**
```bash
# Option 1: Fast refresh
watch -n 0.2 nvidia-smi

# Option 2: Device monitoring (shows utilization over time)
nvidia-smi dmon -s u

# Option 3: Continuous stats
nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv -l 1
```

**Why:** PPO uses GPU in bursts during training phase. 1-second averages miss the peaks!

---

## Expected GPU Utilization

### Single Job (MLP Policy)

| Config | Rollout Phase | Training Phase | Average |
|--------|---------------|----------------|---------|
| Fast | ~5% | ~60% | ~15% |
| Heavy | ~10% | ~80% | ~25% |
| Beast | ~15% | ~95% | ~35% |

**This is NORMAL for RL!** Most time is spent in CPU-bound rollout.

### Parallel Jobs (4x Saturation)

Run 4 jobs simultaneously to overlap phases:

```bash
# Terminal 1
python -m mission_gym.scripts.train_ppo --preset heavy --run-name job1 &

# Terminal 2
python -m mission_gym.scripts.train_ppo --preset heavy --run-name job2 &

# Terminal 3
python -m mission_gym.scripts.train_ppo --preset heavy --run-name job3 &

# Terminal 4
python -m mission_gym.scripts.train_ppo --preset heavy --run-name job4 &
```

**Result:** 4 Ã— 25% = **70-90% sustained GPU utilization!**

---

## Troubleshooting

### "Out of Memory"

Reduce batch size or network size:
```bash
--batch-size 2048 --network-arch "512,512,256"
```

### "batch_size must divide total_batch"

Use `--batch-size auto` or ensure your batch_size divides `n_envs * n_steps`.

### "Too many CPU cores needed"

Beast preset needs ~16 CPU cores. Use `heavy` instead:
```bash
--preset heavy  # Only needs ~8 cores
```

### Low FPS

Check if CPU-bound:
```bash
htop  # Are all cores at 100%?
```

If yes, reduce `n_envs` or use fewer parallel jobs.

---

## Performance Comparison

### Before Fixes (Old Code)

```
n_envs=48, n_steps=43 (2048/48)
total_batch=2064
batch_size=512
epochs=10
GPU: ~10% average
FPS: ~850
```

### After Fixes (Heavy Preset)

```
n_envs=32, n_steps=256
total_batch=8192
batch_size=4096
epochs=20
GPU: ~25% average (bursts to 80%)
FPS: ~700
```

**Trade-off:** Slightly lower FPS, but:
- âœ… 4x larger batch (better learning)
- âœ… 2x more epochs (more GPU work)
- âœ… 2.5x higher GPU utilization
- âœ… Better gradient estimates
- âœ… More stable training

---

## Best Practices

1. âœ… **Start with `heavy` preset** - good balance
2. âœ… **Monitor with `nvidia-smi dmon -s u`** - see real utilization
3. âœ… **Run 3-4 parallel jobs** - saturate GPU properly
4. âœ… **Use `--subproc`** - true parallelism
5. âœ… **Check CPU usage** - don't oversubscribe cores
6. âœ… **Larger batches > more envs** - better for GPU

---

## Example Workflow

```bash
# 1. Test with fast preset
python -m mission_gym.scripts.train_ppo --preset fast --timesteps 1000000 --run-name test

# 2. Train with heavy preset
python -m mission_gym.scripts.train_ppo --preset heavy --timesteps 50000000 --run-name production

# 3. Monitor GPU
nvidia-smi dmon -s u

# 4. For maximum GPU, run 4 heavy jobs in parallel
for i in {1..4}; do
  python -m mission_gym.scripts.train_ppo --preset heavy --timesteps 50000000 --run-name "parallel-$i" &
done
```

---

**Last Updated:** January 26, 2026 (23:30)  
**GPU:** RTX 4070 (8GB VRAM)  
**Status:** âœ… Tested and optimized
